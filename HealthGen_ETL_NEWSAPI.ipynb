{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final | Extração de Dados I\n",
    "----\n",
    "**Sistema de Monitoramento de Avanços no Campo da Genômica**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f2f2f2; text-align: center; padding: 10px;\">\n",
    "  <h3>Script para implementação da ETL </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%------------------------------------------------------------------------------------------------------%<br>\n",
    "<br>\n",
    "``Autores:`` Andrea Elias, Anthony Heimlich, Éverton Donato, Julia Midori e Luana Kruger  <br>\n",
    "<br>\n",
    "``Instituição:`` ADA Tech<br>\n",
    "<br>\n",
    "``Projeto:`` Santander Coders 2023.2<br>\n",
    "<br>\n",
    "``Descrição:`` Este código implementa um sistema de monitoramento de avanços no campo da genômica. O sistema coleta, analisa e apresenta as últimas notícias relacionadas à genômica e à medicina personalizada. <br>\n",
    "<br>\n",
    "``Repositório GitHub:`` https://github.com/evertondcavalcante/PD-I_Santander_Coders23  <br>\n",
    "<br>\n",
    "\n",
    "%------------------------------------------------------------------------------------------------------%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e90224bf-fc0b-40df-b53a-66d888597a75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import requests as req\n",
    "import datetime as dt\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "import flask\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para extração de dados com a News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0ec6ad-6828-42e2-a4a9-fadbe7ded42a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_data(url, acess_code, searched_word, initial_data = None, final_data = None):\n",
    "    \"\"\"\n",
    "    Realiza extração dos dados da API da NEWSAPI com base dos parâmetros recebidos\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        String do método get da API.\n",
    "\n",
    "    acess_code: string\n",
    "        String com a chave usada para coletar os dados da API.\n",
    "\n",
    "    searched_word: string\n",
    "        String com a palavra a ser buscada na notícia.\n",
    "        \n",
    "    initial_data: date\n",
    "        Data inicial a ser buscada, no formato 'YYYY-MM-DD'.\n",
    "\n",
    "    final_data: date\n",
    "        Data final a ser buscada, 'YYYY-MM-DD'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    response_df: float\n",
    "        DataFrame com o retorno dos dados da API\n",
    "\n",
    "    \"\"\"\n",
    "    if initial_data == None:\n",
    "        initial_data = dt.datetime.now().date() - dt.timedelta(days=1)\n",
    "\n",
    "    if final_data == None:\n",
    "        final_data = dt.datetime.now().date() - dt.timedelta(days=1)\n",
    "\n",
    "    parameters = {\n",
    "        'q': searched_word,\n",
    "        'apiKey': acess_code,\n",
    "        'sortBy': 'publishedAt',\n",
    "        'from': initial_data,\n",
    "        'to': final_data\n",
    "    }\n",
    "\n",
    "    response = req.get(url, params = parameters)\n",
    "    response_dic = response.json()\n",
    "\n",
    "    if response_dic['status'] != 'ok':\n",
    "        messageerror = response_dic['message']\n",
    "        raise Exception(messageerror)\n",
    "        \n",
    "    dict_res = {}\n",
    "    \n",
    "    # Percorrer todas as noticias encontradas no resultado da chamada para criar uma chave única\n",
    "    for item in response_dic['articles']:\n",
    "        chave = item['url'] + item['publishedAt']\n",
    "\n",
    "        # Transformar data em um datetime\n",
    "        publishedAt = dt.datetime.strptime(item['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "        dict_res[chave] = {\n",
    "            \"source\": item['source']['name']\n",
    "            ,\"author\": item['author']\n",
    "            ,\"title\": item['title']\n",
    "            ,\"description\": item['description']\n",
    "            ,\"url\":  item['url']\n",
    "            ,\"urlToImage\": item['urlToImage']\n",
    "            ,\"publishedAt\": publishedAt\n",
    "            ,\"content\": item['content']\n",
    "        }\n",
    "\n",
    "    response_df = ps.DataFrame.from_dict(dict_res, orient='index').reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "    print(\"Dados extraídos com sucesso\")\n",
    "    \n",
    "    return response_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para carregar os dados extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5ebd75-1834-4fe8-8945-e2c3c88d0887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data(new_data, path, file_name, key):\n",
    "    \"\"\"\n",
    "    Realiza a leitura do arquivo com os dados já salvo e adiciona os novos dados\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    new_data: DataFrame\n",
    "        Dataframe contendo os dados coletados da API.\n",
    "\n",
    "    path: string\n",
    "        Diretório dentro do dbfs onde os dados serão salvos.\n",
    "\n",
    "    file_name: string\n",
    "        Nome do arquivo em formato parquet onde os dados serão salvos\n",
    "\n",
    "    key: string\n",
    "        Nome do campo que identifica o registro como único\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    # Caso já exista o arquivo transformado, segue direto com a concatenação e com a carga do resultado final\n",
    "    try: \n",
    "        complete_path = path + file_name\n",
    "        arquivo = dbutils.fs.ls(complete_path)\n",
    "\n",
    "        df_res = ps.read_parquet(complete_path)\n",
    "        df_res = ps.concat([df_res, new_data])\n",
    "        # df_res.to_parquet(complete_path)\n",
    "\n",
    "        # elimina os registros duplicados baseado na chave passada antes de armazenar o resultado final\n",
    "        df_res = df_res.sort_values(by=\"publishedAt\")\n",
    "        df_res = df_res.drop_duplicates(subset=key, keep='last')\n",
    "\n",
    "        # armazena o resultado final\n",
    "        df_res.to_parquet(complete_path)\n",
    "        message = \"Carga realizada com sucesso\"\n",
    "\n",
    "    # Caso o arquivo transformando ainda não exista, quer dizer que é o primeiro processo do pipeline e é preciso criar o arquivo destino\n",
    "    except Exception as e:\n",
    "        # caso o arquivo destino não exista, faz a escrita direta do resultado \n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(\"Arquivo não encontrado, primeiro processamento\")\n",
    "            new_data.to_parquet(complete_path)\n",
    "            message = \"Carga realizada com sucesso\"\n",
    "        else:\n",
    "            message = \"Erro na carga: \" + str(e)\n",
    "\n",
    "    print(message)\n",
    "\n",
    "    # return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para verificação do diretório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814b78da-3ec9-469d-ae6a-c07bcee6541d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_folder(path):\n",
    "    \"\"\"\n",
    "    Realiza verificação de existência do diretório onde os dados serão salvos\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        Diretório dentro do dbfs onde os dados serão salvos.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #Verifica se o diretório passado existe. Caso não, cria\n",
    "        dbutils.fs.ls(path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(\"Diretório não existente. Criando diretório.\")\n",
    "            dbutils.fs.mkdirs(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para transformação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(path, file_name):\n",
    "\n",
    "    complete_path = path + file_name\n",
    "\n",
    "    df_new = ps.read_parquet(complete_path)\n",
    "\n",
    "    # Garantir que os anos estão dentro de uma faixa \"normal\" e que não é um \"bad data\"\n",
    "    year_range_mask = df_new[\"publishedAt\"].dt.year>=2024\n",
    "    df_new = df_new[year_range_mask]\n",
    "\n",
    "    # Extrair ano, mês e dia\n",
    "    df_new[\"year_published\"] = df_new[\"publishedAt\"].dt.year\n",
    "    df_new[\"month_published\"] = df_new[\"publishedAt\"].dt.month\n",
    "    df_new[\"day_published\"] = df_new[\"publishedAt\"].dt.day\n",
    "\n",
    "\n",
    "    # Quantidade de notícias por ano, mês e dia de publicação\n",
    "    df_grouped1 = df_new.groupby([\"year_published\", \"month_published\", \"day_published\"]).agg(quantidade_noticias=('id', 'count'))\n",
    "    df_grouped1 = df_grouped1.reset_index(drop=False) # reinicia o index e não exclui o anterior\n",
    "\n",
    "\n",
    "    # Quantidade de notícias por fonte e autor\n",
    "    df_grouped2 = df_new.groupby([\"source\", \"author\"]).agg(quantidade_noticias=('id', 'count'))\n",
    "    df_grouped2 = df_grouped2.reset_index(drop=False)\n",
    "\n",
    "\n",
    "    # Quantidade de aparições de 3 palavras chaves por ano, mês e dia de publicação \n",
    "    keywords = [\"cancer\", \"DNA\", \"genetic\"]   # Insira as palavras chaves que deseja procurar\n",
    "    for keyword in keywords:\n",
    "        df_new[keyword + \"_contagem\"] = df_new[\"content\"].str.count(keyword)\n",
    "\n",
    "    df_grouped3 = df_new.groupby([\"year_published\", \"month_published\", \"day_published\"])[[keyword + \"_contagem\" for keyword in keywords]].sum()\n",
    "    df_grouped3 = df_grouped3.reset_index(drop=False)\n",
    "\n",
    "\n",
    "\n",
    "    # Salvar dados transformados\n",
    "    df_grouped1.to_parquet(path + \"qtd_noticias_ano_mes_dia.parquet\")\n",
    "    df_grouped2.to_parquet(path + \"qtd_noticias_fonte_autor.parquet\")\n",
    "    df_grouped3.to_parquet(path + \"qtd_keywords.parquet\")\n",
    "\n",
    "    return \"Transformações realizadas com sucesso\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elt(url, acess_code, searched_word, path, file_name, key):\n",
    "\n",
    "    print(\"Inicializa o ELT\")\n",
    "\n",
    "    try: \n",
    "        # Verifica se existe novos dados para serem processados\n",
    "        arquivo = dbutils.fs.ls(path)\n",
    "        message = \"Arquivos novos encontrados\"\n",
    "        print(message)\n",
    "\n",
    "        # Faz a chamada da extração, da carga e da transformação dos dados\n",
    "        response_df = extract_data(url, acess_code, searched_word)\n",
    "        check_folder(path)\n",
    "        load_data(response_df, path, file_name, key)\n",
    "        transform_data(path, file_name,)\n",
    "\n",
    "        message = \"ETL realizada com sucesso\"\n",
    "\n",
    "    except Exception as e: # caso não exista nenhum dado novo, retorna com a mensagem e encerra o processo\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            message = \"Nenhum dado novo\"\n",
    "            print(message)\n",
    "        else:\n",
    "            message = \"erro no ELT:\" + str(e)\n",
    "            print(message)\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição dos parâmetros para a execução final do processamento em lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072efcdb-c35f-4be3-879e-081420aba00a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Definir o diretório onde será armazenado o arquivo Parquet final que conterá os dados carregados\n",
    "\n",
    "path = \"/dbfs/data_newsapi/\"\n",
    "file_name = \"data_newsapi.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dados de acesso a News API\n",
    "\n",
    "url = \"https://newsapi.org/v2/everything?\"\n",
    "acess_code = \"ed08e4049379498ebde784dee9d1ede8\"\n",
    "searched_word = \"cancer OR DNA OR genetic\"\n",
    "key = \"id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Códigos úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover pasta\n",
    "# dbutils.fs.rm(path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webhook aguardando a chamada de uma nova requisição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização da aplicação Flask\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "# Definição da rota \"/webhook\" com suporte a requisições HTTP POST\n",
    "@app.route(\"/webhook\", methods=[\"POST\"])\n",
    "\n",
    "def handle_webhook():\n",
    "    # Recupera o conteúdo da requisição como um dicionário em Python\n",
    "    data = flask.request.get_json()\n",
    "    \n",
    "    # Imprime o conteúdo da requisição\n",
    "    print(\"Received data:\", data)\n",
    "    \n",
    "    message = elt(url, acess_code, searched_word, path, file_name, key)\n",
    "\n",
    "    # Retorna uma resposta HTTP simples\n",
    "    return \"message\"\n",
    "\n",
    "# Verifica se o script está sendo executado como um módulo principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Inicia a execução da aplicação\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes de Execução "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae51c816-ab18-4a56-9587-85bd43ce4fac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:467: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo não encontrado, primeiro processamento\n",
      "resultado carregado com sucesso\n"
     ]
    }
   ],
   "source": [
    "## Extração e Carga\n",
    "\n",
    "response_df = extract_data(url, acess_code, searched_word)\n",
    "check_folder(path)\n",
    "load_data(response_df,path,file_name,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dados extraídos\n",
    "response_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dados carregados\n",
    "df_load = ps.read_parquet(path + file_name)\n",
    "df_load.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformação\n",
    "\n",
    "transform_data(path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dados transformados\n",
    "df_transform1 = ps.read_parquet(path + \"qtd_noticias_ano_mes_dia.parquet\")\n",
    "df_transform2 = ps.read_parquet(path + \"qtd_noticias_fonte_autor.parquet\")\n",
    "# df_transform3 = ps.read_parquet(path + \"qtd_keywords.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz a chamada do ELT\n",
    "elt(url, acess_code, searched_word, path, file_name, key)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HealthGen_ETL_NEWSAPI",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
